{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "from meteostat import Stations, Daily, Hourly\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Strava activity data as JSON response\n",
    "auth_url = \"https://www.strava.com/oauth/token\"\n",
    "activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\n",
    "\n",
    "payload = {\n",
    "    'client_id': \"<CLIENT_ID_HERE>\",\n",
    "    'client_secret': '<CLIENT_SECRET_HERE>',\n",
    "    'refresh_token': '<REFRESH_TOKEN_HERE>',\n",
    "    'grant_type': \"refresh_token\",\n",
    "    'f': 'json'\n",
    "}\n",
    "\n",
    "# First API call to request access token\n",
    "# Access tokens expire, so this saves you manual work of regenerating access tokens manually\n",
    "print(\"Requesting Token...\\n\")\n",
    "res = requests.post(auth_url, data=payload, verify=False)\n",
    "access_token = res.json()['access_token']\n",
    "print(\"Access Token = {}\\n\".format(access_token))\n",
    "\n",
    "# Make API request to retrieve the last 400 activities\n",
    "header = {'Authorization': 'Bearer ' + access_token}\n",
    "param1 = {'per_page': 200, 'page': 1}\n",
    "param2 = {'per_page': 200, 'page': 2}\n",
    "param3 = {'per_page': 200, 'page': 3}\n",
    "param4 = {'per_page': 200, 'page': 4}\n",
    "my_dataset1 = requests.get(activites_url, headers=header, params=param1).json()\n",
    "my_dataset2 = requests.get(activites_url, headers=header, params=param2).json()\n",
    "my_dataset3 = requests.get(activites_url, headers=header, params=param3).json()\n",
    "my_dataset4 = requests.get(activites_url, headers=header, params=param4).json()\n",
    "\n",
    "print(my_dataset1[0][\"name\"])\n",
    "print(my_dataset1[0][\"map\"][\"summary_polyline\"])\n",
    "\n",
    "print(my_dataset2[0][\"name\"])\n",
    "print(my_dataset2[0][\"map\"][\"summary_polyline\"])\n",
    "\n",
    "print(my_dataset3[0][\"name\"])\n",
    "print(my_dataset3[0][\"map\"][\"summary_polyline\"])\n",
    "\n",
    "print(my_dataset4[0][\"name\"])\n",
    "print(my_dataset4[0][\"map\"][\"summary_polyline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten nested JSON response into a Dataframe\n",
    "activities1 = pd.json_normalize(my_dataset1)\n",
    "activities2 = pd.json_normalize(my_dataset2)\n",
    "activities3 = pd.json_normalize(my_dataset3)\n",
    "activities4 = pd.json_normalize(my_dataset4)\n",
    "\n",
    "# Combine into one dataframe\n",
    "activities = pd.concat([activities1, activities2, activities3, activities4])\n",
    "activities = activities.reset_index()\n",
    "activities = activities.drop(columns=[\"index\"])\n",
    "\n",
    "print(\"Dataframe shape: \", activities.shape, \"\\n\")\n",
    "print(\"Dataframe columns: \\n\", activities.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag race activities\n",
    "# only keep run activties\n",
    "running_activities = activities.loc[activities[\"type\"] == \"Run\"]\n",
    "running_activities = running_activities.drop(columns=[\"type\"])\n",
    "running_activities[\"race\"] = False\n",
    "\n",
    "# manually mark race activities as \"race\"=True\n",
    "# Races: 11-11-2023 , 2-19-2023, 1-22-2023, 2-27-2022, 1-23-2022, 5-6-2023, 11-24-2022, 12-4-2022\n",
    "# Relays: 3-28-2021, 3-27-2021, 3-27-2021, 3-26-2023, 3-25-2023, 3-25-2023, 8-26-2023, 8-25-2023, 8-25-2023\n",
    "# NOT in current data pull -  1-23-2022, 2-27-2022, 3-28-2021, 3-27-2021, 3-27-2021\n",
    "race_activity_ids = [10270133740,10201538669,8589932304,8434045492,9022328776,8162552673,8207198428]\n",
    "relay_activity_ids = [8779880223,8777939900,8776825449,9721567283,9719999180,9725419043]\n",
    "\n",
    "# uncomment to locate race activities and retrieve ids\n",
    "#display(running_activities.loc[(pd.to_datetime(running_activities[\"start_date_local\"])).dt.date == date(2023,11,23)])\n",
    "#display(running_activities.loc[running_activities.id == 10201538669])\n",
    "\n",
    "for id in race_activity_ids:\n",
    "        running_activities.loc[running_activities.id == id, \"race\"] = True        \n",
    "\n",
    "race_activities = running_activities.loc[running_activities[\"race\"] == True]\n",
    "display(race_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def ft_to_meters(dist_ft):\n",
    "    return dist_ft * 0.3048\n",
    "\n",
    "def miles_to_meters(dist_miles):\n",
    "    return dist_miles * 1609.34\n",
    "\n",
    "def z_score(df, col_name):\n",
    "    df2 = df.copy()\n",
    "    df2[col_name] = ( (df[col_name] - df[col_name].mean()) / df[col_name].std() )\n",
    "    return df2\n",
    "\n",
    "def normalize(df, col_name):\n",
    "    df2 = df.copy()\n",
    "    df2[col_name] = ( (df2[col_name] - df2[col_name].min()) / (df2[col_name].max() - df2[col_name].min()) )\n",
    "    return df2\n",
    "\n",
    "def seconds_to_timedelta(seconds):\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "\n",
    "    return str(\"%d:%02d:%02d\" % (hours, minutes, seconds))\n",
    "\n",
    "def min_per_mile(seconds, pace):\n",
    "    \n",
    "    return\n",
    "    \n",
    "    \n",
    "# Check distribution of numeric columns to decide proper encoding method\n",
    "def plot_elev_high(df):\n",
    "    df[\"elev_high\"].plot(kind=\"hist\", title=\"Elevation High Distribution\")\n",
    "    print(\"elev_high mean:\",df[\"elev_high\"].mean(),\"\\nelev_high std distribution:\",df[\"elev_high\"].std())\n",
    "    \n",
    "def plot_avg_temp(df):\n",
    "    df[\"avg_temp\"].plot(kind=\"hist\", title=\"Average Temperature Distribution\")\n",
    "    print(\"avg_temp mean:\",df[\"avg_temp\"].mean(),\"\\navg_temp std distribution:\",df[\"avg_temp\"].std())\n",
    "    \n",
    "def plot_avg_cadence(df):\n",
    "    df[\"average_cadence\"].plot(kind=\"hist\", title=\"Average Cadence Distribution\")\n",
    "    print(\"average_cadence mean:\",df[\"average_cadence\"].mean(),\"\\naverage_cadence std distribution:\",df[\"average_cadence\"].std())\n",
    "\n",
    "def plot_max_hr(df):\n",
    "    df[\"max_heartrate\"].plot(kind=\"hist\", title=\"Maximum Heartrate Distribution\")\n",
    "    print(\"max_heartrate mean:\",df[\"max_heartrate\"].mean(),\"\\nmax_heartrate std distribution:\",df[\"max_heartrate\"].std())\n",
    "\n",
    "def plot_avg_hr(df):   \n",
    "    df[\"average_heartrate\"].plot(kind=\"hist\", title=\"Average Heartrate Distribution\")\n",
    "    print(\"average_heartrate mean:\",df[\"average_heartrate\"].mean(),\"\\naverage_heartrate std distribution:\",df[\"average_heartrate\"].std())\n",
    "\n",
    "def plot_max_speed(df):\n",
    "    df[\"max_speed\"].plot(kind=\"hist\", title=\"Maxiumum Speed Distribution\")\n",
    "    print(\"max_speed mean:\",df[\"max_speed\"].mean(),\"\\nmax_speed std distribution:\",df[\"max_speed\"].std())\n",
    "\n",
    "def plot_elev_gain(df):\n",
    "    df[\"total_elevation_gain\"].plot(kind=\"hist\", title=\"Total Elevation Gain Distribution\")\n",
    "    print(\"total_elevation_gain mean:\",df[\"total_elevation_gain\"].mean(),\"\\ntotal_elevation_gain std distribution:\",df[\"total_elevation_gain\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nulls(df, method):\n",
    "    if method == \"interpolation\":\n",
    "        df = df.interpolate()\n",
    "        return df\n",
    "\n",
    "def handle_nulls_race(input_df, training_df):\n",
    "    race_activities = training_df.loc[training_df[\"race\"] == True]\n",
    "    display(input_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves weather data from a given set of locations (loc_df) across a date range (min_date -> max_date)\n",
    "# Returns input timestamped dataframe (df) with an additional column, avg_temp\n",
    "def get_weather(min_date, max_date, loc_df, df):\n",
    "\n",
    "    # Get unique list of weather stations from locations\n",
    "    print(\"Retrieving list of weather stations...\")\n",
    "    weather_stations = pd.DataFrame()\n",
    "    loc_dict = {}\n",
    "    for location in loc_df:\n",
    "        stations = Stations()\n",
    "        nearby_station = stations.nearby(location[0], location[1]).fetch(1)\n",
    "        loc_dict[(\"[ \"+str(location[0])+\" \"+str(location[1])+\"]\")] = nearby_station.index[0]\n",
    "        weather_stations = pd.concat([weather_stations, nearby_station])\n",
    "\n",
    "    weather_stations = weather_stations.drop_duplicates(subset='name')\n",
    "    print(\"Weather stations retrieved.\")\n",
    "    display(weather_stations.head())\n",
    "    \n",
    "    # Retrieving Weather Data from Nearby Stations\n",
    "    # Create dictionary to hold dataframes of weather data. Dict key is weather station id.\n",
    "    print(\"Retrieving weather data...\")\n",
    "    weather_df_dict = {}\n",
    "    for coord, station_id in loc_dict.items():\n",
    "        weather_df_dict[station_id] = pd.DataFrame()\n",
    "    \n",
    "    # Retrieve at least a year of weather data\n",
    "    today = date.today()\n",
    "    last_year = today.year - 1\n",
    "    a_year_ago_today = date(last_year, today.month, today.day)\n",
    "    if a_year_ago_today < min_date:\n",
    "        min_date = a_year_ago_today\n",
    "\n",
    "    for idx, station in weather_stations.iterrows():\n",
    "        # Daily Weather\n",
    "        weather_df_dict[idx] = Daily(idx, start=datetime.combine(min_date, datetime.min.time()), end=datetime.combine(max_date, datetime.min.time())).fetch()\n",
    "\n",
    "        # Hourly Weather\n",
    "        #weather_df_dict[idx] = Hourly(idx, start=datetime.combine(min_date, datetime.min.time()), end=datetime.combine(max_date, datetime.min.time())).fetch()\n",
    "    print(\"Weather data retrieved.\")\n",
    "    \n",
    "    # Add weather data to loc_df\n",
    "    print(\"Adding weather data to df...\")\n",
    "    \n",
    "    # Add weather data to run_activities dataframe\n",
    "    df[\"weather_station\"] = None\n",
    "    df[\"avg_temp\"] = None\n",
    "\n",
    "    for idx, activity in df.iterrows():\n",
    "        dict_key = \"[ \" + str(activity['start_lat']) + \" \" + str(activity['start_lng']) + \"]\"\n",
    "        if dict_key in loc_dict:\n",
    "            weather_df = weather_df_dict[loc_dict[dict_key]]\n",
    "            # check if date is in the future\n",
    "            if today < activity[\"start_date_local\"]:\n",
    "                # if date is in future, use weather from last year\n",
    "                new_date = date(activity[\"start_date_local\"].year - 1, activity[\"start_date_local\"].month, activity[\"start_date_local\"].day)\n",
    "                try:\n",
    "                    df.at[idx,\"weather_station\"] = loc_dict[dict_key]\n",
    "                    df.at[idx,\"avg_temp\"] = weather_df.loc[str(new_date)][\"tavg\"]\n",
    "                except:\n",
    "                    # weather not found for date / station\n",
    "                    df.at[idx,\"weather_station\"] = loc_dict[dict_key]\n",
    "                    df.at[idx,\"avg_temp\"] = np.nan\n",
    "            try:\n",
    "                df.at[idx,\"weather_station\"] = loc_dict[dict_key]\n",
    "                df.at[idx,\"avg_temp\"] = weather_df.loc[str(activity[\"start_date_local\"])][\"tavg\"]\n",
    "            except:\n",
    "                # weather not found for date / station\n",
    "                df.at[idx,\"weather_station\"] = loc_dict[dict_key]\n",
    "                df.at[idx,\"avg_temp\"] = np.nan\n",
    "    \n",
    "    print(\"Weather data added.\")\n",
    "    df = df.drop(labels=[\"weather_station\"], axis=1)\n",
    "    display(df.head())\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns df with encoded categorical and numeric columns.\n",
    "def data_encoding(df, skip_encoding):\n",
    "    df_encoded = df\n",
    "    \n",
    "    # encode time cyclically\n",
    "    # split date time into seperate columns\n",
    "    df_encoded.loc[:,\"date-time\"] = pd.to_datetime(df_encoded.start_date_local.astype(str)+ \" \" + df_encoded.start_time.astype(str ))\n",
    "    df_encoded[\"year\"] = df_encoded[\"date-time\"].dt.year\n",
    "    df_encoded[\"month\"] = df_encoded[\"date-time\"].dt.month\n",
    "    df_encoded[\"day\"] = df_encoded[\"date-time\"].dt.day\n",
    "    df_encoded[\"hour\"] = df_encoded[\"date-time\"].dt.hour\n",
    "    df_encoded[\"min\"] = df_encoded[\"date-time\"].dt.minute\n",
    "    df_encoded[\"sec\"] = df_encoded[\"date-time\"].dt.second\n",
    "    df_encoded[\"total_seconds\"] = df_encoded[\"hour\"]*60*60 +  df_encoded[\"min\"]*60 +  df_encoded[\"sec\"]\n",
    "    \n",
    "    # cyclically encode date and time with sin / cos functions\n",
    "    seconds_in_day = 24*60*60\n",
    "\n",
    "    df_encoded[\"month_sin\"] = np.sin(2*np.pi*df_encoded.month/12)\n",
    "    df_encoded[\"month_cos\"] = np.cos(2*np.pi*df_encoded.month/12)\n",
    "\n",
    "    df_encoded[\"day_sin\"] = np.sin(2*np.pi*df_encoded.day/31)\n",
    "    df_encoded[\"day_cos\"] = np.cos(2*np.pi*df_encoded.day/31)\n",
    "\n",
    "    df_encoded[\"sec_sin\"] = np.sin(2*np.pi*df_encoded.total_seconds/seconds_in_day)\n",
    "    df_encoded[\"sec_cos\"] = np.cos(2*np.pi*df_encoded.total_seconds/seconds_in_day)\n",
    "    \n",
    "    # one-hot encode year\n",
    "    one_hot_year = pd.get_dummies(df_encoded[\"year\"])\n",
    "    df_encoded = df_encoded.join(one_hot_year)\n",
    "    \n",
    "    if not skip_encoding:\n",
    "        # normalize or standardize numeric columns based on distribution of values (normal or not)\n",
    "        df_encoded = z_score(df_encoded, \"average_cadence\")\n",
    "        df_encoded = z_score(df_encoded, \"average_heartrate\")\n",
    "        df_encoded = z_score(df_encoded, \"max_heartrate\")\n",
    "        df_encoded = normalize(df_encoded, \"total_elevation_gain\")\n",
    "        df_encoded = normalize(df_encoded, \"elev_high\")\n",
    "        df_encoded = z_score(df_encoded, \"max_speed\")\n",
    "        df_encoded = z_score(df_encoded, \"avg_temp\")\n",
    "\n",
    "    \n",
    "    # drop uncoded columns\n",
    "    df_encoded = df_encoded.drop(columns=[\"date-time\",\"year\",\"month\",\"day\",\"hour\",\"min\",\"sec\",\"total_seconds\",\"start_time\",\"start_date_local\"])\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, skip_encoding=False):\n",
    "    \n",
    "    # modify date/time/location columns\n",
    "    df['start_date_local'] = pd.to_datetime(df['start_date_local'])\n",
    "    df['start_time'] = df['start_date_local'].dt.time\n",
    "    df['start_date_local'] = df['start_date_local'].dt.date\n",
    "    df['start_lat'] = round(df['start_latlng'].str[0], 2)\n",
    "    df['start_lng'] = round(df['start_latlng'].str[1], 2)\n",
    "    df = df.drop(columns=['start_latlng'])\n",
    "    \n",
    "    # add weather data\n",
    "    # if date is in future (i.e. for race prediction), get the weather from that location last year\n",
    "    min_date, max_date = df['start_date_local'].min(), df['start_date_local'].max()\n",
    "    locations = df.loc[:,['start_lat', 'start_lng']].drop_duplicates().values\n",
    "    \n",
    "    df = handle_nulls(df, \"interpolation\")\n",
    "    df = get_weather(min_date, max_date, locations, df)\n",
    "    df[\"avg_temp\"].fillna((df[\"avg_temp\"].mean()), inplace = True) # handle any null values in new avg_temp col\n",
    "    df = df.drop(columns=['start_lat','start_lng'])\n",
    "    print(\"How many nulls?\") # verify there are no null values\n",
    "    print(df.isnull().sum())\n",
    "    df = data_encoding(df, skip_encoding)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predicts moving time / average pace during a race based on race's location, elevation, and distance.\n",
    "def race_predictor(model_training_df, input_param_dict, model_name):\n",
    "    # Verify format of input parameters - race_date, start_latlng, distance, elev_gain, elev_high\n",
    "    if len(input_param_dict) < 5:\n",
    "        print(\"Missing input parameters. race_predictor is expecting a dictionary with keys: race_date, start_latlng, distance, elev_gain, elev_high.\")\n",
    "        return\n",
    "    \n",
    "    cols = ['start_date_local', 'start_latlng', 'distance', 'moving_time',  'total_elevation_gain', 'max_speed',\n",
    "            'average_cadence', 'average_heartrate', 'max_heartrate', 'elev_high', 'race']\n",
    "    model_training_df = model_training_df[cols]\n",
    "    race_activities_df = model_training_df.loc[model_training_df[\"race\"] == True]\n",
    "    \n",
    "    # populate input_df with avg values from other race activities\n",
    "    input_df = pd.DataFrame( {\n",
    "        \"start_date_local\": [input_param_dict[\"race_date\"]],\n",
    "        \"start_latlng\": [input_param_dict[\"start_latlng\"]],\n",
    "        \"distance\":  [miles_to_meters(input_param_dict[\"distance_miles\"])],\n",
    "        \"moving_time\": [None],\n",
    "        \"total_elevation_gain\": [ft_to_meters(input_param_dict[\"elev_gain_ft\"])],\n",
    "        \"max_speed\": [race_activities_df[\"max_speed\"].mean()],\n",
    "        \"average_cadence\": [race_activities_df[\"average_cadence\"].mean()],\n",
    "        \"average_heartrate\": [race_activities_df[\"average_heartrate\"].mean()],\n",
    "        \"max_heartrate\": [race_activities_df[\"max_heartrate\"].mean()],\n",
    "        \"elev_high\": [ft_to_meters(input_param_dict[\"elev_high_ft\"])],\n",
    "        \"race\": True,\n",
    "        \"prediction\": True}\n",
    "                            \n",
    "        )\n",
    "    model_training_df[\"prediction\"] = False\n",
    "    print(\"Model Training DF:\")\n",
    "    display(model_training_df)\n",
    "\n",
    "    # Create single df before pre-processing\n",
    "    model_df = pd.concat([model_training_df, input_df])\n",
    "    training_data = data_preprocessing(model_df)\n",
    "    \n",
    "    if model_name == \"linear_reg\":\n",
    "        x_race = training_data.loc[training_data[\"prediction\"] == True]\n",
    "        x_train = training_data.loc[training_data[\"prediction\"] == False]\n",
    "        \n",
    "        df = x_train.drop(columns=[\"moving_time\",\"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"])\n",
    "        x = np.array(x_train.drop(columns=[\"moving_time\",\"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"]))\n",
    "        y = np.array(x_train[\"moving_time\"])\n",
    "        \n",
    "        # Build Multiple Linear Regression model with scikit-learn\n",
    "        model = LinearRegression().fit(x,y)\n",
    "        \n",
    "        # Print model results\n",
    "        r_sq = model.score(x,y)\n",
    "        print(f\"coefficient of determination: {r_sq}\")\n",
    "        print(f\"intercept: {model.intercept_}\")\n",
    "        print()\n",
    "        print(f\"coefficients: {model.coef_}\")\n",
    "\n",
    "        # summarize feature importance\n",
    "        importance = model.coef_\n",
    "        for i,v in enumerate(importance):\n",
    "            print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "        \n",
    "        # plot feature importance\n",
    "        plt.bar([x for x in range(len(importance))], importance)\n",
    "        plt.show()\n",
    "        \n",
    "        # Use model to predict moving_time for race\n",
    "        x_pred = np.array(x_race.drop(columns=[\"moving_time\",\"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"]))\n",
    "    \n",
    "        y_pred = model.predict(x_pred)\n",
    "        \n",
    "        print(\"prediciton:\",y_pred)\n",
    "        \n",
    "        print(seconds_to_timedelta(y_pred))\n",
    "\n",
    "    elif model_name == \"nn\":\n",
    "\n",
    "        device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "        print(\"Using {} device for nn training\".format(device))\n",
    "\n",
    "        # Define Neural Network\n",
    "        class NeuralNetwork(nn.Module):\n",
    "            def __init__(self, input_size):\n",
    "                super().__init__()\n",
    "                self.linear_relu_stack = nn.Sequential(\n",
    "                    nn.Linear(input_size, 15),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(15,30),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(30,45),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(45,60),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(60,45),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(45,30),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(30,15),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(15,1),\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = x.to(torch.float32)\n",
    "                logits = self.linear_relu_stack(x)\n",
    "                return logits\n",
    "                \n",
    "        training_data['moving_time'] = training_data['moving_time'].astype(float)\n",
    "        x_race = training_data.loc[training_data[\"prediction\"] == True]\n",
    "        x_train = training_data.loc[training_data[\"prediction\"] == False]\n",
    "        \n",
    "        \n",
    "        x = np.array(x_train.drop(columns=[\"moving_time\"]))\n",
    "        y = np.array(x_train[\"moving_time\"])\n",
    " \n",
    "        #TODO: incorporate dataloader\n",
    "        \n",
    "        model = NeuralNetwork(len(x[0]))\n",
    "        model.to(device)\n",
    "        opt = torch.optim.Adam(model.parameters())\n",
    "        loss_func = nn.MSELoss()\n",
    "\n",
    "        epochs = 10\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Starting epoch {epoch}....\")\n",
    "            for x_item, y_item in zip(x, y):\n",
    "                x_item = np.asarray(list(x_item))\n",
    "                x_item = torch.from_numpy(x_item)\n",
    "                y_item = torch.from_numpy(np.asarray([float(y_item)]))\n",
    "                y_item_pred = model(x_item)\n",
    "                loss = loss_func(y_item_pred, y_item.to(torch.float32))\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        \n",
    "        x_race_arr = np.array(x_race.drop(columns=[\"moving_time\"]))\n",
    "        x_race_item = np.array(list(x_race_arr[0]))\n",
    "        x_race_item = torch.from_numpy(x_race_item) #TODO: fix\n",
    "        y_race_pred = model(x_race_item)[0] # might have to feed in zeroth index\n",
    "        moving_time_prediction = seconds_to_timedelta(y_race_pred.item())\n",
    "        \n",
    "        \n",
    "    return \"Predicted race time for {distance} mile race is {pred}\".format(distance=input_param_dict[\"distance_miles\"], pred=moving_time_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training data sets\n",
    "marathon_test_stop = date(2023,11,11)\n",
    "marathon_test_stop_idx = (running_activities.loc[running_activities.index[(pd.to_datetime(running_activities[\"start_date_local\"])).dt.date == marathon_test_stop]]).index[0]\n",
    "fifteenK_stop = date(2023,11,22)\n",
    "fifteenK_stop_idx = (running_activities.loc[running_activities.index[(pd.to_datetime(running_activities[\"start_date_local\"])).dt.date == fifteenK_stop]]).index[0]\n",
    "\n",
    "marathon_test_activities = running_activities[marathon_test_stop_idx : -1]\n",
    "fifteenK_test_activities = running_activities[fifteenK_stop_idx : -1]\n",
    "\n",
    "display(marathon_test_activities.head())\n",
    "display(fifteenK_test_activities.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models on upcoming races\n",
    "richmond_marathon_input_dict = {\n",
    "        \"race_date\": \"2023-11-11T07:00:00Z\",\n",
    "        \"start_latlng\": [37.5407, -77.4360],\n",
    "        \"distance_miles\": 26.2,\n",
    "        \"elev_gain_ft\": 490,\n",
    "        \"elev_high_ft\": 272\n",
    "}\n",
    "\n",
    "seattle_15k_input_dict = {\n",
    "        \"race_date\": \"2023-11-23T09:30:00Z\",\n",
    "        \"start_latlng\": [47.6798, -122.2536],\n",
    "        \"distance_miles\": 9.32,\n",
    "        \"elev_gain_ft\": 342,\n",
    "        \"elev_high_ft\": 67\n",
    "}\n",
    "\n",
    "# Uncomment to test different models\n",
    "race_predictor(marathon_test_activities, richmond_marathon_input_dict, \"linear_reg\")\n",
    "#race_predictor(fifteenK_test_activities, seattle_15k_input_dict, \"linear_reg\")\n",
    "#race_predictor(marathon_test_activities, richmond_marathon_input_dict, \"nn\")\n",
    "#race_predictor(fifteenK_test_activities, seattle_15k_input_dict, \"nn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "race_pred_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
